\chapter{Implementierung}

Die Implementierung beschäftigt sich mit den Details zur Umsetzung der Feature Extraktion, sowie des Bag of Visual Words und Autoencoders.

\section{Feature Extraktion}

Zur Extraktion der Features wird die OpenCV SIFT Implementierung verwendet. Zur Verwendung von SIFT, ist es erforderlich das opencv Projekt zusammen mit dem opencv-contrib Projekt selbst zu kompilieren. Bei SIFT handelt es sich um einen patentierten Algorithmus, daher ist er seit Version 3.0 nicht mehr standardmäßig im opencv Projekt enthalten [REF]. Zu Gewinnung der keypoints eines Bildes, muss ein neuen SIFT Detektor erstellt und die detect Methode mit einem Bild aufgerufen werden.

\lstset{language=C}
\begin{lstlisting}
#import <opencv2/core/core.hpp>
#import <opencv2/highgui/highgui.hpp>
#import <opencv2/nonfree/features2d.hpp>

using namespace cv;

int main () {
	const Mat image = imread(IMAGE_PATH, 0);
	SiftFeatureDetector detector;
	vector<KeyPoint> keypoints;
	
	detector.detect(input, keypoints);
}
\end{lstlisting}

TODO: Platzhalter Code-Listing, aber im Grunde funktioniert's so.

\section{Bag of Visual Words}

Das Bag of Visual Words Modell wurde direkt in C und cuda umgesetzt. Zur Erstellung und Interaktion eines Modells dient die Klasse BoVW und bietet hierfür vier Funktionen an. Die Funktion createCodebook(float **features) baut ein Codebook aus einer gegebenen Menge von Features auf und speichert dieses intern. Durch die Funktion saveCodebook(String path) und loadCodebook(String path) kann ein intern gespeichertes Codebook in eine Datei geschrieben werden bzw. von einer Datei eingelesen werden. Die letzte Funktion classify(String imagePath, int count) extrahiert lokale Features eines Bildes und berechnet die Ähnlichkeit der Visual Words. Es werden die ähnlichsten Bilder zurückgegeben. Die Anzahl der Ergebnisse kann durch den Parameter \textit{count} gesteuert werden. 
Da es sich bei der Berechnung der Cluster sowie der relativen Häufigkeiten um die rechenintensivsten Funktionen handelt, wurde diese parallelisiert.

\subsection{Paralleler k-means Algorithmus}

Die Umsetzung der k-means Algorithmus ist direkt in cuda erfolgt. Als Referenzimplementierung diente hier das Projekt von [REF]. Der Algorithmus erwartet die einen Feature-Vektor, die Feature Dimension und die Anzahl der zu bildenden Cluster als Eingabe. Vor dem Kernelaufruf wird für die Features und Cluster der notwendige Speicher allokiert und die Daten zum \textit{device} kopiert. Die Dimensionen der Features sowie der Cluster werden durch \textit{float} Werte dargestellt, sodass das ein SIFT Feature-Vektor beispielsweise 128 $*$ 4 Byte = 512 Byte belegt. 

\subsection{Histogramm als parallele Reduktion}

In der Konzeption wurde aufgezeigt, wie sich die Berechnung eines Histogramms parallelisieren lässt, da die Operation assoziativ und kommutativ ist: Es spielt keine Rolle in welcher Reihenfolge die Daten abgearbeitet werden bzw. in welcher Reihenfolge die Klassen inkrementiert werden. Wenn das zu beschreibende Histogramm im global Speicher vorliegt, wird die Berechnungsgeschwindigkeit stark reduziert, da viele Threads auf die gleichen Speicheradressen des Histogramms schreibend zugreifen. Damit es nicht zu Lese- / Schreibanomalien kommt, muss das Inkrementieren einer Klasse atomar sein, d.h. zwischen Lese- und Schreibzugriff darf kein anderer Thread auf die Adresse zugreifen. Dies wird in cuda durch die Operation \textit{atomicAdd} realisiert. Damit die Anzahl an Threads die auf dieselbe Adresse schreiben eingeschränkt wird, arbeitet jeder Block auf einem lokalen Histogramm im \textit{shared memory}. Wenn alle Blöcke ihre lokalen Histogramme berechnet haben, müssen diese noch in das Histogramm im \textit{global memory} kumuliert werden.

\lstset{language=C}
\begin{lstlisting}
__global__
void histogram_kernel (float *buffer, long size, int *histo, int bins) {
	extern __shared__ int *copy[];
	
	if (threadIdx.x < bins) {
		copy[threadIdx.x] = 0;		
	}
	__syncthreads();

	int id = threadIdx.x + blockDim.x * gridDim.x;
	int stride = blockDim.x * gridDim.x;
	
	while (i < stride) {
		int bin = buffer[i] / bins; 
		atomicAdd(&(copy[bin]), 1);
		i += stride;	
	}
	__syncthreads();
	
	if (threadIdx.x < bins) {
		atomicAdd(&(histo[threadIdx.x]), copy[threadIdx.x]);		
	}
}
\end{lstlisting} 

\begin{itemize}
	\item TODO: Das ist so ziemlich die Variante aus der Vorlesung, muss durch Variante mit features /clustern ersetzt werden.
\end{itemize}

\section{Autoencoder}

Zur Implementierung des Autoencoders wurde TensorFlow verwendet. TensorFlow ist ein DeepLearning Framework und bietet Schnittstellen in diversen Sprachen an. Neben OpenCL wird auch Nvidias cuda unterstützt, sodass TensorFlow Programme automatisch von Grafikkarten profitieren können, ohne das der Entwickler diese explizit berücksichtigen muss. Für diese Umsetzung eines Autoencoders wurde Python und das Projekt \textit{libsdae-autoencoder-tensorflow}\footnote{https://github.com/rajarsheem/libsdae-autoencoder-tensorflow} von Rajar Sheem genutzt. Ein simpler Autoencoder mit einem HiddenLayer lässt sich wie folgt definieren:

\lstset{language=Python}
\begin{lstlisting}
from deepautoencoder import StackedAutoEncoder

model = StackedAutoEncoder(dims=[3], activations=['relu'], epoch=[1000], loss='rmse', lr=0.007, batch_size=50, print_step=100)
                       
result = model.fit_transform(x)
\end{lstlisting}

\begin{enumerate}
	\item TODO: erklären, tatsächliches Netz
	\item Referenzlayout
	\item Einlesen der Feature Vektoren etc.
	\item Speichern / Laden eines Netzes?
	\item Training
\end{enumerate}