\chapter{Konzept}

Das Kapitel Konzeption beschäftigt sich zunächst mit dem Prozess der Feature Extraktion. Hierfür wird der SIFT Algorithmus nach Lowe genutzt. Folgend wird das Bag of Visual Words Modell näher betrachtet: Es werden auf Basis der Analyse parallele Varianten des Clustering und Histogramm Algorithmus entworfen, die sich zur Ausführung auf SPMD Architekturen eignen. Anschließend wird der geplante Ablauf zum Generieren eines Bag of Visual Word Modells und Labelling eines Modells durch das Modell erläutert.
Abschließend wird auf der Basis der Arbeit von [REF] ein Autoencoder eingeführt, der aus SIFT \textit{keypoints} eine Darstellung des Features in nur 36 Dimensionen lernt.

\section{Feature Extraktion}

Die Extraktion der Features ist die Basis für beide Varianten der Klassifizierung. Das Bag of Visual Words Modell nutzt die von SIFT erzeugten Feature Deskriptor für die weitere Verarbeitung. Der Autoencoder hingegen arbeitet mit Gradienten der \textit{keypoints} die vom SIFT Detektor ermittelt wurden. Aus diesem Grund werden die Feature-Vektoren und \textit{keypoints} beide berechnet und getrennt gespeichert.
Der SIFT Deskriptor enthält 128 Dimensionen und ist so für einen Vergleich nur schwer geeignet, da pro Bild ca. 100 bis 1000 Feature Vektoren generiert werden. Es werden daher im folgenden zwei Ansätze vorgestellt, die die Dimensionalität der Features reduzieren und eine durch Grafikkarten gestützte Berechnung von ähnlichen Bildern ermöglichen.

\todo{Was noch hier, oder überhaupt?}

\section{Ansatz 1: Bag of Visual Words}

In der Analyse wurde bereits sequentielle Varianten des Lloyd und Histogramm Algorithmus vorgestellt und aufgezeigt, an welchen Stellen eine Parallelisierung der Berechnung durch Grafikkarten erfolgen kann. Im Folgenden wird aus diesen Informationen je Algorithmus eine parallele Version für SPMD Prozessoren abgeleitet.
In den beiden nachfolgenden Abschnitten Generierung des Modells und Labeling eines Bildes wird auf den Programmaufbau und -ablauf näher eingegangen. Es werden die wesentlichen Funktionen, ihre Parameter und Aufrufe skizziert.

\subsection{Parellisierung von Llyods Algorithmus}

Der Thread in einem Block mit der ID 0 fungiert hier als Master für die anderen Threads. Die Initialisierung der Cluster mit zufälligen Vektoren aus $v$ wird ebenfalls von diesem übernommen. Die Zuweisung von Vektoren zu Clustern nimmt $\Theta(nk)$ Zeit in Anspruch, wobei $n$ die Anzahl Vektoren und $k$ die Anzahl der Cluster ist. Diese Phase kann parallelisiert werden, in dem pro Feature Vektor ein Thread verwendet wird: Jeder Thread berechnet für seinen Feature Vektor die Distanz zu allen Clusterschwerpunkten und bestimmt den Index des Clusters, der am Nächsten ist. Dieser Prozess ist in Pseudocode in Zeile 6 bis 8 ausgedrückt. Bevor die Cluster aktualisiert werden, müssen die Threads synchronisiert werden: Andernfalls ist nicht garantiert, dass die Berechnung jedes Threads abgeschlossen ist.

\lstset{language=C}
\begin{lstlisting}[mathescape=true]
kmeans_gpu
	if threadId == 0
		$c_{j} = rand(p_{i}) \in P, \: j = 1,...,k, \: c_{j} \neq c_{i} \: \forall i \neq j$
	synchronize threads
	until convergence
		for each $x_{i} \in P_{threadId}$
			$l_{i} = argminD(c_{j}, p_{i})$
		synchronize threads
		if threadId == 0
			for each $p_{i} \in P$
				$c_{l_{i}} = c_{l_{i}} + p_{i}$
				$m_{l_{i}} = m_{l_{i}} + 1$
			for each $c_{j} \in C$
				$c_{j} = \frac{1}{m_{j}} c_{i}$
\end{lstlisting}

\subsection{Parallele Reduzierung von Histogrammen}

\todo{Allgemeines parallel reduction Prinzip in SIMD (Pseudocode?)}

Die Berechnung eines Histogramms kann hervorragend parallelisiert werden, da die Operation assoziativ und kommutativ ist: Es spielt keine Rolle in welcher Reihenfolge die Daten abgearbeitet werden bzw. in welcher Reihenfolge die Klassen inkrementiert werden. Wenn das zu beschreibende Histogramm im global Speicher vorliegt, wird die Berechnungsgeschwindigkeit stark reduziert, da viele Threads auf die gleichen Speicheradressen des Histogramms schreibend zugreifen. Damit es nicht zu Lese- / Schreibanomalien kommt, muss das Inkrementieren einer Klasse atomar sein, d.h. zwischen Lese- und Schreibzugriff darf kein anderer Thread auf die Adresse zugreifen. Dies wird in CUDA durch die Operation \textit{atomicAdd} realisiert. Damit die Anzahl an Threads die auf dieselbe Adresse schreiben eingeschränkt wird, arbeitet jeder Block auf einem lokalen Histogramm im \textit{shared memory}. Wenn alle Blöcke ihre lokalen Histogramme berechnet haben, müssen diese noch in das Histogramm im \textit{global memory} kumuliert werden.

\lstset{language=C}
\begin{lstlisting}
__global__
void histogram_kernel (float *buffer, long size, int *histo, int bins) {
	extern __shared__ int *copy[];
	
	if (threadIdx.x < bins) {
		copy[threadIdx.x] = 0;		
	}
	__syncthreads();

	int id = threadIdx.x + blockDim.x * gridDim.x;
	int stride = blockDim.x * gridDim.x;
	
	while (i < stride) {
		int bin = buffer[i] / bins; 
		atomicAdd(&(copy[bin]), 1);
		i += stride;	
	}
	__syncthreads();
	
	if (threadIdx.x < bins) {
		atomicAdd(&(histo[threadIdx.x]), copy[threadIdx.x]);		
	}
}
\end{lstlisting} 

\subsection{Aufbau des Bag of Visual Words Algorithmus}

Das Bag of Visual Words Modell soll zwei Anwendungsfälle unterstützen. Zunächst muss aus einer Menge von Bildern ein Modell generiert werden. Da verschiedene Modelle erstellt werden sollen, müssen diese gespeichert und auch wieder eingelesen werden können. Der Abschnitt Generierung des Modells beschäftigt sich mit einem Entwurf solch eines Systems. Sofern ein Modell generiert wurde, soll es einem Anwender möglich sein ein neues Bild anhand des Modells zu labeln. Dieser Prozess ist in Kapitel Labeling eines Bildes dargestellt.

\subsubsection{Generierung des Modells}

Die Generierung eines Modells kann durch die Funktion \textit{generateModel} gestartet werden. Als Parameter werden der Pfad für die Bilddaten \textit{imageDir}, der Zielpfad \textit{modelPath} und die Anzahl der Cluster \textit{k} erwartet. Der Ablauf der folgenden Funktionsaufrufe ist in Abbildung \ref{img:concept_bovw_1} dargestellt. Im ersten Schritt wird \textit{extractFeatures} aufgerufen um alle SIFT-Features der Bilder, die in \textit{imageDir} enthalten sind, zu extrahieren. Als nächstes werden durch \textit{clusterFeatures} die Features in \textit{k} Cluster gruppiert. Die Berechnung der Cluster, der Distanzen von Features zu Clustern und des Konvergenzkriteriums erfolgt durch die GPU. Als Ergebnis werden die $k$ berechneten Schwerpunkte der Cluster und die Mitgliedschaft der Features zurückgegeben. Abschließend speichert \textit{saveModel} die Cluster unter \textit{$<modelPath>/clusters$} und die Mitgliedschaft unter \textit{$<modelpath>/membership$}. \todo{single extractFeature}

\begin{figure}
	\centering
	\includegraphics[scale=0.8]{images/concept_bovw_1.png}
	\caption{Funktionen zur Generierung eines Modells}
	\label{img:concept_bovw_1}
\end{figure}

\subsubsection{Labeling eines Bildes}

Sofern ein Modell erstellt wurde, können auf dessen Basis Bilder verglichen werden. In Abbildung \ref{img:concept_bovw_2} sind schematisch die aufeinanderfolgenden Funktionsaufrufe dargestellt. Die Funktion \textit{getImageLabels} wird mit dem Pfad des Modells und des zu vergleichenden Bildes aufgerufen. Das Modell wird durch \textit{readModel} eingelesen und die Clusterschwerpunkte initialisiert. Die SIFT Features werden, wie bei der Generierung, durch \textit{extractFeatures} ermittelt. Die Funktion \textit{selectLabels} berechnet durch \textit{computeFrequencies} das Histogramm der Visual Words aus den Cluster und Features auf der GPU. Auf dieser Basis werden dann die top X Labels ermittelt und zurückgegeben.

\begin{figure}
	\centering
	\includegraphics[scale=0.8]{images/concept_bovw_2.png}
	\caption{Funktionen zur Gewinnung von Labels eines Bildes}
	\label{img:concept_bovw_2}
\end{figure}

\section{Ansatz 2: Autoencoder}

In diesem Ansatz wird zur Reduzierung der Dimensionen der Feature-Vektoren ein Stacked Denoising Autoencoder verwendet wie er in der Arbeit von Zhao \cite{aed2016} vorgeschlagen wurde. Der Autoencoder soll eine komprimierte Darstellung der Gradientenvektoren erzielen, die aus den \textit{intereset points} berechnet werden. Da dieser Vektor 3042 Werte enthält, besitzt der Autoencoder in der Eingabeschicht 3042 Neuronen. Der Encoder des vorgeschlagenen Modells besteht aus fünf Schichten, deren Neuronenanzahl sukzessive reduziert wird, bis schließlich eine Darstellung in 36 Dimensionen erreicht wird. Abbildung \ref{img:ae_model} zeigt die Schichten des Autoencoders. In der Arbeit wurde bereits aufgezeigt, dass der modellierte Autoencoder \textit{state of the art} Ergebnisse erzielt: Die Ergebnisse des Autoencoders wurden unter verschiedenen Kriterien mit den Ergebnissen der SIFT-PCA und TODO Methode verglichen. Dabei erkannte der Autoencoder in fast allen vielen die gleichen Features, jedoch durch einen 36 statt 128 dimensionalen Feature-Vektor. Theoretisch wird hier also das gleiche Ergebnis in einem Drittel der Zeit ermittelt.

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{images/ae_model.png}
	\caption{Schichten des verwendeten Autoencoders \cite{aed2016}}
	\label{img:ae_model}
\end{figure}

\todo{Mit Referenz auf die Arbeit ist es plausibel dieses Modell zu verwenden?}

\todo{In der Konzeption bereits TensorFlow erwähnen und somit Beschleunigung durch cuda Bindings?}