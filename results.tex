\chapter{Evaluierung}

Die Qualität der Ergebnisse des in der Implementierung realisierten Modells wird in diesem Kapitel durch ein Testexperiment überprüft. Weiterhin werden Ausführungszeiten für beispielsweise verschieden große $k$ beim Bag of Visual Words betrachtet, um eine Einsicht in die Performance der Algorithmen zu gewinnen.
Im Kapitel Experimentaufbau wird zunächst das Experiement sowie verwendete Metriken und Ergebnistypen behandelt. Nachfolgend wird erläutert, wie die großen Menge an benötigten Trainings- und Testdaten wiederholbar und automatisiert aufgebaut wird. Dieser Abschnitt illustriert auch, wie eine Durchführung des Experiments vonstatten geht. Im letzten Teil werden dann konkrete Testgruppen aus den Caltech101 \cite{cal2004} Bilddaten erzeugt. Diese Menge an Bilddaten hat große Verbreitung im Bereich der Objekterkennung gefunden. Auf diese Weise ist ein Vergleich mit Arbeiten Anderer prinzipiell möglich.

\section{Testdaten und Testgenerierung}

Der Abschnitt Testdaten stellt die hier verwendete Menge von Bildern vor, die Caltech101, welche extra für den Test von Algorithmen bezüglich der Objekterkennung in Bildern entwickelt wurde.\newline
Im folgenden Abschnitt zur Testgenerierung wird ein Verfahren zur zufälligen Auswahl von Trainings- und Testbildern unter Berücksichtigung verschiedener Restriktionen, wie z.B. dem Verhältnis der Anzahl von Trainings- zu Testbildern vorgestellt.

\subsection{Testdaten}

Als Testmenge wurden die Bilder der Caltech101-Menge verwendet. Bei Caltech101 handelt es sich um eine weit verbreitete Menge von Bilddaten, die vorwiegend zum Test von Algorithmen bezüglich der Objekterkennung in Bildern dient. Insgesamt liegen, wie der Name sagt, 101 Kategorien vor, die jeweils zwischen 40 und 800 Bildern enthalten. Auf der offiziellen Webseite \footnote{http://www.vision.caltech.edu/Image\textunderscore Datasets/Caltech101/} und im Artikel der Autoren wird empfohlen, die eigene Arbeit mit derer anderen vergleichbar zu halten, indem:

\begin{itemize}
	\item Eine feste Anzahl an Trainings- und Testbildern verwendet wird.
	\item Experimente mit einer zufälligen Auswahl an Bildern wiederholt werden.
	\item Ähnlich viele Bilder, wie in den Arbeiten anderer, verwendet werden (1, 3, 5, 10, 15, 20 oder 30 Trainingsbilder; 20 oder 30 Testbilder).
\end{itemize}

Die Caltech101 Daten liegen nach Download kategorisiert im JPG-Format vor. Die Struktur wurde so beibehalten und ist noch für die Testgenerierung relevant. Direkt unter dem Caltech101-Ordner ist pro Kategorie ein Ordner vorhanden, der die jeweiligen Bilder immer im gleichen Namensschema enthält:\newline

\dirtree{%src
.1 Caltech101.
.2 accordion.
.3 image\textunderscore 0001.jpg.
.3 image\textunderscore 0002.jpg.
.3 ....
.2 airplanes.
.3 ....
.2 ....
}

Abbildung \ref{img:strawberries} zeigt vier Bilder aus der Kategorie \enquote{Erdbeere}. Neben Bildern von realen Rosengewächsen sind auch Zeichnungen und Objekte enthalten, die Form und Farbe der Erdbeere nachempfunden sind. Auch sind die Objekte auf einem Bild zum Teil in unterschiedlicher Menge vorhanden. Durch diese Variation ist ein Algorithmus so gefordert, tatsächlich eine Abstraktion zu lernen.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{images/strawberry.png}
	\caption{Verschiedene Bilder aus der Kategorie \enquote{Erdbeere} der Caltech101 Bilddaten.}
	\label{img:strawberries}
\end{figure}

\subsection{Testgenerierung}

Die Generierung von Testdaten bietet sich aus mehreren Gründen an. Zum einen sind enorm viele Trainingsdaten notwendig, um ein leistungsfähiges Modell zu generieren, zum anderen sollen im Test ca. 2000 Bildpaare verwendet werden. Ein manueller Testaufbau wäre fehleranfällig und nicht sehr flexibel. Da ein praktisch taugliches Modell erst durch die Variation einiger Parameter gefunden werden kann, ist es wünschenswert, Testdaten mit verschiedenen Eigenschaften generieren zu können:

\begin{itemize}
	\item Die Anzahl der Kategorien sollte bestimmbar sein. Dies entspricht einer Kategorie der Caltech101-Daten. Somit sind hier theoretisch bis zu 101 Kategorien im Test denkbar.
	\item Das Verhältnis bzw. die Anzahl an Trainings- und Testbildern muss definierbar sein.
\end{itemize}

Letztendlich sollen die Histogramme der \textit{Visual Words} zweier Bilder miteinander verglichen und so die Ähnlichkeit gemessen werden. Ein Programm automatisiert daher die Generierung solcher Paare: Es werden zufällige Bildpaare aus ausgewählten Kategorien (\textit{airplanes}, \textit{anchor}, ...) selektiert. Diese Paare, sowie die Information, ob die Bilder in der selben oder einer verschiedenen Kategorie liegen, stellen einen Testkandidaten dar. Zwei Bilder liegen dabei in der selben Klasse, wenn sie im selben Ordner im Dateisystem, also hier im Caltech101-Ordner, enthalten sind. Das Ergebnis wird dann als Datei \textit{test \textunderscore time.txt} gespeichert. Die Pfade der Bilder werden hierbei relativ zum Caltech101-Ordner gespeichert, die Information über die Kategorie wird als \enquote{+} bzw. \enquote{-} kodiert. Eine Datei für die Kategorien \textit{airplanes} und \textit{anchor} könnte dann so beginnen:\newline

\begin{lstlisting}
airplanes/image_0023.jpg airplanes/image_0009.jpg +
airplanes/image_0002.jpg anchor/image_0015.jpg -
anchor/image_0013.jpg airplanes/image_0002.jpg -
anchor/image_0001.jpg anchor/image_0005.jpg +
airplanes/image_0006.jpg anchor/image_0006.jpg -
...
\end{lstlisting}

Neben den zu verwendenden Kategorien muss bei Erzeugung die Anzahl der Testkandidaten, das Verhältnis von positiven zu negativen Kategorien sowie die Anzahl der Trainings- und Testbilder angegeben werden. \newline
Die Bilder, welche durch das Programm für das Training ausgewählt wurden, werden separat als \textit{train\textunderscore time.txt} gespeichert. Pro Zeile ist hier der relative Pfad des Bildes innerhalb des Caltech101-Ordners enthalten.

\section{Experimentaufbau}

Das Experiment soll sowohl die Feature-Kompression durch einen Autoencoder testen als auch die Kategorisierung bzw. den Vergleich der Bilder durch den Bag of Visual Words. Aus diesem Grund ist das Experiment zweigeteilt: 

\begin{enumerate}[(a)]%
	\item In dieser Variante findet ein reiner Test des Bag of Visual Words statt. Hierfür werden durch SIFT die Feature-Deskriptoren von Trainingsbildern extrahiert und direkt als Eingabe an den Bag of Visual Words gegeben. Anschließend folgt die Verarbeitung der Testbilder.
	\item Hier wird nach Extraktion der \textit{keypoints} durch den SIFT-Detektor der Feature-Deskriptor durch den Autoencoder erzeugt. Die so erhaltenen Features werden dann wie in (a) durch den Bag of Visual Words gruppiert und anschließend die \textit{Visual Words} der Testbilder erzeugt.
\end{enumerate}

Wichtig ist, dass pro Durchführung des Experiments in beiden Varianten die gleichen Trainings- und Testbilder verwendet werden, damit die Ergebnisse beider Deskriptoren miteinander vergleichbar sind. \newline
Nach Erzeugung des Modells mit den Trainingsbildern, werden nun die Features der Testkandidaten extrahiert und pro Bild dies \textit{Visual Words} berechnet. Die Ähnlichkeit \textit{sim (similarity)} der resultierenden Histogramme $h_1$ und $h_2$ wird dann als 1 $-$ \textit{MSE (mean squared error)} gemessen:

$$sim(h_1, h_2) = 1 - MSE(h_1, h_2)$$
$$MSE(h_1, h_2) = \frac{1}{n}\sum_{i=0}^{n}(h_{1_i} - h_{2_i})^{2}$$

Beträgt die Ähnlichkeit von zwei Histogrammen 1 werden sie als identisch angesehen. Werte nahe 0 drücken aus, dass die Histogramme sich sehr voneinander unterscheiden. Damit nun unterschieden werden kann, ob zwei Bilder in derselben Klasse sind, muss die Ähnlichkeit mit einem Schwellwert verglichen werden. So kann beispielsweise definiert werden, dass es sich bei einer Ähnlichkeit größer als $0.8$ um dieselbe Klasse handelt. Somit hat der Schwellwert unmittelbar Auswirkungen auf die Ergebnisse und sollte selbst Bestandteil des Experiments sein. Das Resultat eines solchen Vergleichs ist dann einer der beiden folgenden Kategorien zuzuordnen:

\begin{itemize}
	\item \textbf{True Positives} Bei \textit{True Positives} handelt es sich um zwei Bildern die entweder in der gleichen oder einer verschiedenen Klasse liegen und die Vorhersage des Modells diesbezüglich korrekt ist.
	\item \textbf{False Positives} In diesem Fall ist die Klassifizierung durch das Modell nicht korrekt: Bei gleicher Klasse wurde eine geringe Ähnlichkeit erkannt, bei verschiedenen eine Hohe.
\end{itemize}

Damit ein Modell zuverlässige Ergebnisse liefert, muss es größtenteils \textit{True Positives} erkennen, bzw. der Anteil der \textit{True Positives} sollte im Verhältnis zu den \textit{False Positives} bei weitem überwiegen. Für eine visuelle Darstellung dieses Verhältnisses eignet sich die \textit{Receiver Operating Characteristic (ROC)}: Diese stellt die \textit{True Positives} auf der Ordinate und die \textit{False Positives} auf der Abzisse dar.

\section{Experimentdurchführung}

Das Clustering wurde in drei Experimenten getestet. Pro Experiment wurden Eigenschaften einer Bildmenge variiert, um zu beobachten, wie sich diese Veränderungen auf die Qualität der Ergebnisse auswirkt. Die Testmengen sind jeweils wie folgt gewählt:

\begin{itemize}
	\item \textbf{Test 1} besteht aus zwei Kategorien: \enquote{Bonsai} und \enquote{Leopard}. Es wurden 50 Bilder verwendet, aus beiden Kategorien je 25. Von diesen Bildern sind 30\% für die Trainings- und die anderen 70\% für die Testphase bestimmt. Im Training liegen ca. $10.000$ Features vor, im Test knapp $27.000$.
	\item \textbf{Test 2} enthält ebenfalls, wie Test 1, die Kategorien \enquote{Bonsai} und \enquote{Leopard}. Hier werden aber insgesamt 100 Bilder verwendet. Aus beiden Kategorien wurden wieder gleich viele Bilder gewählt und die Aufteilung in Trainings- bzw. Testdaten bleibt auch gleich. Hier wurden für das Training ca. $21.000$ Features extrahiert und für den Test $55.000$.
	\item \textbf{Test 3} enthält drei Kategorien von Bildern, um zu testen, ob eine komplexere Kategorisierung prinzipiell möglich ist. Als Kategorien wurden hier \enquote{Flugzeug}, \enquote{Motorrad} und \enquote{Armbanduhr} gewählt. Insgesamt werden 80 Bilder verwendet, jede Kategorie steuert ein Drittel der Bilder bei. Die Aufteilung in Trainings- und Testdaten wird auch hier beibehalten. In der Trainingsphase lagen ungefähr $18.000$ Features vor, in der Testphase ca. $52.000$.
\end{itemize} 

Als Grafikkarte für die Experimente wurde eine Nvidia Quadro M4000 (GM204GL) verwendet. Bei dieser Grafikkarte stehen 48kB \textit{shared memory} zur Verfügung, sodass bei 1024 Threads 91 Cluster für SIFT und ca. 300 Cluster für die Autoencoder-Features möglich sind. Die Programme wurden unter CUDA 7.5 ausgeführt.\newline

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/roc_ph.png}
	\caption{ROC-Kurven für die SIFT-Features und verschiedene $k$ aus Experiment 1.}
	\label{img:roc1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/roc_ph.png}
	\caption{ROC-Kurven für die Autoencoder-Features und verschiedene $k$ aus Experiment 1.}
	\label{img:roc2}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/roc_ph.png}
	\caption{ROC-Kurven für die SIFT-Features und verschiedene $k$ aus Experiment 2.}
	\label{img:roc3}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/roc_ph.png}
	\caption{ROC-Kurven für die Autoencoder-Features und verschiedene $k$ aus Experiment 2.}
	\label{img:roc4}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/roc_ph.png}
	\caption{ROC-Kurven für die SIFT-Features und verschiedene $k$ aus Experiment 2.}
	\label{img:roc5}
\end{figure}

\begin{figure}
	\centering
    \includegraphics[scale=0.4]{images/roc_ph.png}
    \caption{ROC-Kurven für die Autoencoder-Features und verschiedene $k$ aus Experiment 2.}
    \label{img:roc6}
\end{figure}

%\begin{tabular}[t]{| r | c | c |}
% \hline
% k 	 & AUC  \\ \hline    
%   		5    & 0.00 \\ \hline
 %   	10   & 0.00 \\ \hline
  %  	20   & 0.00 \\ \hline
   % 	50   & 0.00 \\ \hline
    %	100  & 0.00 \\ \hline
	%	250  & 0.00 \\ \hline
	%	500  & 0.00 \\ \hline
	%\end{tabular}


Für die Experimente ist in den Abbildungen \ref{img:roc1} bis \ref{img:roc6} das Ergebnis der Klassifizierung der SIFT- und Autoencoder-Features dargestellt. Dabei handelt es sich um das Verhältnis der richtig erkannten Kategorien (\textit{True Positives}) zu falsch erkannten Kategorien (\textit{False Negatives}). Hieraus ist ersichtlich, dass das Clustering sowohl durch SIFT als auch der Autoencoder Features nicht erfolgreich war: Ein Kurve nahe der $45$\degree  Linie sagt aus, dass das Verhältnis zwischen \textit{TP} und \textit{FP} ausgeglichen ist. Die Fläche unter der ROC-Kurve beträgt in Experiment 1 für SIFT, je nach $k$, zwischen \todo{x\%} und \todo{y\%}. Das ist bei zwei Kategorien also kaum besser als \enquote{raten}. Ein ähnliches Bild zeichnet sich bei der gleichen Testgruppe, jedoch durch den Autoencoder komprimiert. Mit einer Präzision zwischen \todo{x\%} und \todo{y\%} ist auch hier für kein gewähltes $k$ eine erfolgreiche Klassifizierung durchgeführt worden.\newline
Auch bei Verwendung einer größeren Trainings- und Testmenge (Abbildung \todo{ref} für SIFT, \todo{ref} für den Autoencoder) verbessern sich die Ergebnisse nicht nennenswert. 
Auffallend ist durch alle Experimente auch, dass bei zu großem $k$ ($> 200$), bei den vorliegenden Testdaten und zwei bzw. drei Kategorien die Klassifizierung zunehmend schlechter ausfällt. Ein Grund hierfür könnte sein, dass nicht genug \enquote{einzigartige} Features vorhanden sind: Ein Cluster, der ein semantisches Merkmal abbildet, wird durch ein größeres $k$ möglicherweise aufgeteilt.

\begin{table}
    \hfill
    \begin{tabular}[t]{| r | c | c |}
    \hline
	     & SIFT & AE \\ \hline    
    5    & 0.00 & 0.00 \\ \hline
    10   & 0.00 & 0.00 \\ \hline
    20   & 0.00 & 0.00 \\ \hline
    50   & 0.00 & 0.00 \\ \hline
    100  & 0.00 & 0.00 \\ \hline
	250  & 0.00 & 0.00 \\ \hline
	500  & 0.00 & 0.00 \\ \hline
	1000 & 0.00 & 0.00 \\ \hline  
    \end{tabular}
    \hfill
    \begin{tabular}[t]{| r | c | c |}
    \hline
	     & SIFT & AE \\ \hline    
    5    & 0.00 & 0.00 \\ \hline
    10   & 0.00 & 0.00 \\ \hline
    20   & 0.00 & 0.00 \\ \hline
    50   & 0.00 & 0.00 \\ \hline
    100  & 0.00 & 0.00 \\ \hline
	250  & 0.00 & 0.00 \\ \hline
	500  & 0.00 & 0.00 \\ \hline
	1000 & 0.00 & 0.00 \\ \hline    
    \end{tabular}
    \hfill
    \begin{tabular}[t]{| r | c | c |}
    \hline
	     & SIFT & AE \\ \hline    
    5    & 0.00 & 0.00 \\ \hline
    10   & 0.00 & 0.00 \\ \hline
    20   & 0.00 & 0.00 \\ \hline
    50   & 0.00 & 0.00 \\ \hline
    100  & 0.00 & 0.00 \\ \hline
	250  & 0.00 & 0.00 \\ \hline
	500  & 0.00 & 0.00 \\ \hline
	1000 & 0.00 & 0.00 \\ \hline    
    \end{tabular}
    \hfill
	\caption{Laufzeiten der \textit{global memory} Implementierung des Bag of Visual Words in Sekunden. Von links nach rechts ist Test 1 bis 3 aufgelistet.}
\end{table}

\begin{table}
    \hfill
    \begin{tabular}[t]{| r | r | r |}
    \hline
	     & SIFT & AE \\ \hline    
    5    &  1.20 & 0.00 \\ \hline
    10   &  4.37 & 0.00 \\ \hline
    20   & 12.09 & 0.00 \\ \hline
    50   & 35.13 & 0.00 \\ \hline
    90   & 43.01 & 0.00 \\ \hline
	200  & -     & 0.00 \\ \hline
	300  & -     & 0.00 \\ \hline    
    \end{tabular}
    \hfill
    \begin{tabular}[t]{| r | r | r |}
    \hline
	     & SIFT & AE \\ \hline    
    5    &   6.94 & 0.00 \\ \hline
    10   &   7.63 & 0.00 \\ \hline
    20   &  17.42 & 0.00 \\ \hline
    50   &  37.01 & 0.00 \\ \hline
    90   &  54.88 & 0.00 \\ \hline
	200  &      - & 0.00 \\ \hline
	300  & 	    - & 0.00 \\ \hline
	\end{tabular}
    \hfill
    \begin{tabular}[t]{| r | c | c |}
    \hline
	     & SIFT & AE \\ \hline    
    5    & 0.00 & 0.00 \\ \hline
    10   & 0.00 & 0.00 \\ \hline
    20   & 0.00 & 0.00 \\ \hline
    50   & 0.00 & 0.00 \\ \hline
    90   & 0.00 & 0.00 \\ \hline
	200  & -    & 0.00 \\ \hline
	300  & -    & 0.00 \\ \hline
	\end{tabular}
    \hfill
	\caption{Laufzeiten der \textit{shared memory} Implementierung des Bag of Visual Words in Sekunden. Von links nach rechts ist Test 1 bis 3 aufgelistet.}
\end{table}