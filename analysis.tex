\chapter{Analyse}

Ziel der Arbeit ist es, dass ein Anwender die in der HsH Datenbank vorhandenen Bilddaten in eine Menge von $k$ verschiedenen Gruppen einteilen kann, um so semantische Informationen über die Bilder zu gewinnen. Darüber hinaus sollen neu hinzukommende Bilder auf der Basis eines gewonnen Modells klassifiziert werden können. Das $k$ kann hierbei vom Anwender vorgegeben werden um verschiedene Gruppen zu erhalten und so unterschiedliche Zusammenhänge zwischen den Bildern zu entdecken. 
In diesem Rahmen sollen zwei Methoden miteinander verglichen werden, die eine Klassifikation von Bildern auf Basis von Feature Deskriptoren ermöglichen. Eine wesentliche Eigenschaft die der Deskriptor aufweisen sollte, ist affine Invarianz. Gegenwärtig ist aber noch kein Deskriptor vorhanden, der eine Einbeziehung aller möglichen Umstände berücksichtigt. Eine Reihe von geeigneten und verbreiteten Deskriptoren wird im Kapitel \nameref{extraction} vorgestellt und diskutiert.
Ein Vergleich von Bildern anhand von Features ist nicht unmittelbar möglich. Die Feature-Vektoren weisen sehr hohe Dimensionen auf, was eine effiziente Berechnung nicht möglich macht. Die Folgenden beiden Abschnitte behandeln zum Einen das Bag of Visual Words Modell und zum anderen den Autoencoder. Beide Ansätze reduzieren auf unterschiedliche Weise die Dimensionen von Features um so eine Berechnung der Ähnlichkeit zu beschleunigen. 

\begin{itemize}
	\item Ein bischen mehr hier zu BoVW / AE: state of the art, andere Paper...
	\item Ausführung auf GPU berücksichtigen
\end{itemize}

\section{Feature Deskriptoren}
\label{extraction}

Zum Vergleich von Bilddaten müssen Features aus den Bildern extrahiert werden, die diese charakterisieren. In der Literatur finden sich verschiedene Ansätze, die je nach Einsatzgebiet unterschiedliche Stärken besitzen. 
Der Local Binary Pattern (LBP) Deskriptor kodiert die Informationen eines Pixels und seiner Nachbarschaft der Größe $3 \times 3$ Pixel eines Graustufenbildes in einem Histogramm. Der resultierende Feature-Vektor enthält somit 256 Dimensionen. LBP sind gegenüber Rotationen invariant und wurden auch um die Verarbeitung um Bilder in Farbe erweitert. Besonders gute Ergebnisse konnten mit LBP im Bereich der Gesichts- und Texterkennung erzielt werden.
Einen ganz anderen Ansatz haben Torralba und Olivia verfolgt: Statt Objekte durch lokale Features zu beschreiben, werden globale Eigenschaften betrachtet. Das Bild wird in einem Raum mit wenig Dimensionen abgebildet, dem sogenannten \textit{Spatial Envelope}. Die Autoren nutzen hier wahrnehmbare Dimensionen wie Natürlichkeit und Offenheit um den Raum zu definieren. Eine hohe Natürlichkeit weist zum Beispiel auf das Bild einer Landschaft hin: Hier kommen in der Regel kaum gerade vertikale und horizontale Linien vor, im Gegensatz zu Bildern, die von Menschen angefertigt wurden.  Bilder die in einer semantischen Kategorie Ähnlichkeiten aufweisen, liegen dann nah beieinander. Dieses Modell hat sich vor allem bewährt um eine Umgebung zu klassifizieren. \cite{mts2001}
Noch SIFT hier anfangen. Noch ein Deskriptor 

\begin{itemize}
	\item Detaillierter ausführen / Weitere Deskriptoren (SURF, SUSAN) ?
	\item Histogram of Oriented Gradients aufnehmen -> Autoencoder Basis
\end{itemize}

\section{Ansatz 1: Bag of Visual Words}

Im ersten Ansatz soll das Bag of Visual Words Modell verfolgt werden. Zu Beginn liegen die Feature-Vektoren vor, die in der vorigen Phase extrahiert wurden. Um das Codebook aufzubauen ist es erforderlich die Visual Words zu generieren. Als Clustering Algorithmus soll hier Llyods heuristische Variante des k-means Algorithmus verwendet werden. Zunächst wird eine gängige sequentielle Implementierung angeführt, auf deren Basis die Parallelisierbarkeit durch Grafikkarten untersucht wird. Bei der Einordnung eines Features wird ein Histogramm der Visual Words generiert, daher wird im Anschluss ein sequentieller Histogramm Algorithmus vorgestellt, der auf Parallelisierbarkeit geprüft wird.

\subsection{Lloyds Algorithmus}

Im Grundlagenkapitel wurde bereits der allgemeine Ablauf von Lloyds Algorithmus beschrieben, der sich grundlegend in drei Stufen einteilen lässt. Im Nachfolgendem Codelisting ist der Ablauf des Algorithmus in Pseudocode beschrieben. Als Parameter werden die Punkte $P$ und die Anzahl der zu bildenden Cluster $k$ erwartet. In Zeile 2 findet die Auswahl der initialen Schwerpunkte der Cluster statt. Die Zuordnung von Punkten zu Clustern erfolgt in Zeile 7: $argminD$ wählt den Cluster aus, dessen Varianz am wenigsten bei Aufnahme des Punktes $p_{i}$ steigt. Abschließend wird die Aktualisierung der Schwerpunkte aller Cluster in Zeile 9 durchgeführt.

\lstset{language=C}
\begin{lstlisting}[mathescape=true]
kmeans_lloyd ($P, C, k$)
	initialisierung
	until convergence
		$C_{j} = 0, j = 1, ..., k$
		for each $p_{i} \in P$
			for each $c_{j} \in C$
				$c_{j} = argminD(c_{j}, p_{i})$		
		for each $c_{j} \in  C$
			$c_{j} = \frac{1}{|c_{j}|} \sum_{n_{i} \in c_{j}} n_{i}$
\end{lstlisting}

Die Initialisierungsphase muss für die Parallelisierung nicht beachtet werden: Sie nimmt nur wenig Zeit in Anspruch und wird einmalig zu Beginn ausgeführt. Die anderen beiden Schritte des Algorithmus bergen mehr Potential: In Zeile 5 bis 7 wird die Varianz jedes Cluster-Vektor Paars berechnet. Da die Berechnung der Varianz unabhängig von der eines anderen ist, kann die Berechnung aller Varianzen parallel erfolgen. 

\begin{itemize}
	\item theoretisch: http://www.know-center.tugraz.at/download\_extern/papers/latex8.pdf
	\item praktisch: https://github.com/serban/kmeans (siehe Mail)
\end{itemize}

\subsection{Histogramme}

TODO

\section{Ansatz 2: Autoencoder}

Im zweiten Ansatz soll ein Autoencoder genutzt werden um die Dimensionalität des Feature-Vektors zu reduzieren. In der Arbeit von [REF] wurde bereits vorgestellt, wie auf Basis eines Autoencoders ein Bilddeskriptor erzeugt werden kann. Es werden durch den SIFT Detektor die \textit{interest points} eines Bildes ermittelt. Für jeden \textit{intereset Point} werden die lokalen Gradienten in horizontale und vertikale Richtung einer $41 \times 41$ großen Nachbarschaft berechnet. Diese werden in einem Vektor der Größe $2 \times 39 \times 39 = 3042$ gespeichert. Der Encoder besteht aus fünf Stufen, um die Gradienten zu komprimieren.

\begin{enumerate}
	\item Intro Machine Learning, Aufschwung und Verbreitung 
	\item Netzwerke zur Nutzung für beaufsichtigtes Lernen
	\item Autoencoder für unbeaufsichtigtes Lernen
	\item auch hier gibt es ein Paper, das mir viel zum Autoencoder liefert, auch bereits hier erwähnen?
	\item GPU!
\end{enumerate}