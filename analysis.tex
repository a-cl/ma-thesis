\chapter{Analyse}

Ziel dieser Arbeit ist es, ein Modell zu entwickeln, das es ermöglicht große Mengen von Bildern zu gruppieren. Wegen der großen Mengen an Daten, die zu verarbeiten sind, sollen \textit{state of the art} Verfahren genutzt werden, die speziell hierauf ausgelegt sind. Der erste der Teil der Analyse befasst sich daher mit der Betrachtung von geeigneten unüberwachten Lernverfahren für diesen Zweck. Hier wir untersucht, welchen Ansatz andere gewählt haben, um das Problem zu lösen. Auf dieser Basis und anhand der Anforderungen und Annahmen wird dann ein unüberwachtes Lernverfahren ausgewählt, dass zu Gruppierung von Bildern dient und in den folgenden Kapiteln weiter ausgearbeitet und realisiert wird. \newline
Im zweiten Teil sollen Möglichkeiten untersucht werden, aus Bildern Features zu gewinnen, welche als Eingabe für das Modell dienen. Um einen Überblick über Features in der Bildverarbeitung zu gewinnen, werden zunächst einige Feature-Detektoren bzw. Deskriptoren für unterschiedlicher Anwendungsfälle angeführt. Heute ist es kaum vorstellbar, dass ein Feature-Deskriptor jeden möglichen Anwendungsfall abdecken kann. Daher soll abschließend der Fokus dieser Arbeit festgelegt werden: Sollen beispielsweise Gesichter oder Szenen erkannt werden? Sollen Objekte erkannt werden und wenn ja, beliebige Kategorien? Anhand der gewonnen Erkenntnisse wird dann entschieden, welche Eigenschaften der hier verwendete Feature-Deskriptor aufweisen soll. 

\section{Verwendung der GPU}

Zum Aufbauen eines Modells werden mehrere Zehn- bis Hunderttausend Features verarbeitet werden. Viele der Verfahren, die der Erzeugung solcher Modellen zu Grunde liegen, wurden in den vergangenen Jahren durch die Verwendung der GPU statt der CPU beschleunigt. Bei der Betrachtung geeigneter Ansätze wird daher auch berücksichtigt, ob und wie eine Beschleunigung durch parallele Verarbeitung erzielt werden kann. Gerade bei großen Datenmengen und einer enormen Datenparallelität können Probleme durch GPUs um ein vielfaches schneller gelöst werden als durch CPUs. Da Nvidias CUDA an der Hochschule Hannover sowohl gelehrt als auch zu Forschungszwecken genutzt wird und sich CUDA auch international in Forschung und Wirtschaft etabliert hat, soll die Plattform als technische Basis dienen. 

\section{Geeignete unüberwachte Lernverfahren}

Für die Gruppierung des Bildmaterials eignen sich maschinelle Lernverfahren besonders, da sie zum einen auf eine große Menge an Trainingsdaten angewiesen sind, um ein nützliches Modell zu generieren und zum anderen eine parallele Verarbeitung begünstigen. Der Einsatz unüberwachter Lernverfahren ist hier aus folgenden Gründen plausibel:

\begin{itemize}
	\item Das Gruppieren bzw. Kategorisieren von Daten ist ein Teilgebiet der unüberwachten Lernverfahren. Hier wird von Clustering gesprochen.
	\item Die Feature-Vektoren umfassen oft viele Komponenten, welche zur Kodierung der Eigenschaften erforderlich sind. Unüberwachte Lernalgorithmen ermöglichen ein exploratives Vorgehen: Die Feature-Vektoren werden auf ihre wesentlichen Komponenten analysiert und so eine kompaktere Darstellung erzeugt.
	\item Es sollen Strukturen in den Daten entdeckt werden, die nicht a priori bekannt sind. Ein überwachter Ansatz erfordert zum Training \textit{gelabelte} Daten, um Vorhersagen zu treffen. Da die gesuchten Strukturen aber gerade nicht bekannt sind, scheidet ein überwachter Ansatz aus.
\end{itemize}

Das Gruppieren von Vektoren, hier den Bild-Features, kann also durch einen \textit{Clustering} Algorithmus umgesetzt werden. Außerdem scheint es sinnvoll die Features vor dem Clustering aufzubereiten: Durch eine kompaktere Darstellung der Vektoren kann der notwendige Speicher reduziert und die Berechnung beschleunigt werden. Solche Methoden zur Kompression sind sogenannte assoziative Verfahren: Hier werden Strukturen in den Daten gesucht, die nicht offensichtlich sind. Durch die Verwendung dieser unterliegenden Strukturen kann auf einen Teil der ursprünglichen Information verzichtet werden. Im nächsten Abschnitt wird betrachtet, wie sich andere Autoren dieser Fragestellung angenähert haben und welche Vorarbeit bereits geleistet wurde.

\subsection{Verwandte Arbeiten}

Methoden, um großen Mengen Daten zu gruppieren, sind aktuell gefragter denn je: Die zunehmende Vernetzung und Nutzung von Technik im privaten sowie in der Wirtschaft, führt zu großen Datenströmen. Zentrales Element in den Arbeiten andere ist meist ein k-means Algorithmus. Dieser wird hinsichtlich verschiedener Ziele, wie Ausführungszeit oder Präzision, von den Autoren optimiert.  Einige dieser Arbeiten werden im folgenden vorgestellt.\newline
Bevor ein Clustering zum Einsatz kommt, ist es wünschenswert, dass die Daten in kompakter Form vorliegen. Oft sind in den Daten viele Informationen vorhanden, die nicht substantiell sind, also als \enquote{Rauschen } betrachtet werden. Außerdem ist es von Vorteil die Daten in einem Raum mit wenig Dimensionen zu beschreiben, um den \enquote{Fluch der Dimensionalität}\footnote{In diesem Kontext bedeutet das, dass sich, mit zunehmenden Dimensionen, die Distanzen zwischen den Vektoren kaum noch voneinander unterscheiden.} zu vermeiden. Klassische Verfahren wie die Hauptkomponentenanalyse gelangen bei komplexen, nicht lineare Daten an ihre Grenzen. Aus diesem Grund schließt sich der Vorstellung verschiedener Clustering-Verfahren eine Betrachtung von modernen Ansätzen zur Verringerung der Dimensionalität an.\newline

Yedla et al. \cite{ikm2010} haben ihre verbesserte Version des k-means-Algorithmus einfach \enquote{Improved K-Means} getauft. Fokus dieser Arbeit ist eine Methode, die bessere initiale Cluster wählt, als ein Zufallsverfahren. Hierdurch ist es möglich, die Vektoren den Clustern in geringerer Zeit zuzuweisen und die Genauigkeit dabei zu erhöhen (gegenüber dem originale k-means-Algorithmus).\newline
Der sogenannte \enquote{Bag of keypoints} wurde 2004 von Csurka et al. \cite{bok2004} vorgeschlagen. Die Idee lehnt sich an den Bag of Words aus dem Bereich Information Retrival an. Statt Wörtern und Dokumenten sind hier jedoch Bilder und ihre Features Gegenstand. Die Deskriptoren der Features werden quantisiert um so Cluster zu bilden, die jeweils ein semantisches Merkmal bilden sollen. Hierdurch ist es den Autoren in Experimenten gelungen, Bilder mit hoher Genauigkeit korrekt zu kategorisieren.\newline
Der von Seldan et al. \cite{jis2003} entwickelten \enquote{Joint Image Segmentation} liegt ebenfalls die vorige Analogie (Word/Dokument, Bild/Features) zugrunde. Hier werden die Texturen als Features aufgefasst. Ähnliche Bilder sollten viele Texturen gemeinsam haben, daher ist es Ziel dieses Algorithmus, eine Verbindung zwischen ähnlichen Texturen verschiedener Bilder herzustellen. In Experimenten wurde das Verfahren auf natürliche Bilder (Meer, Landschaften) und verschiedene Malstile (van Gogh, Rembrandt, ...) angewendet. Die Autoren demonstrieren, dass in allen Experimenten sinnvolle Cluster gefunden wurden. \newline 
Eine Möglichkeit Datenstrukturen in einem Raum mit geringerer Dimensionalität unüberwacht zu lernen, ist der Einsatz von speziellen neuronalen Netzen, den sogenannten Autoencodern. Das Konzept des Autoencoders reicht zwar bis in die 80er Jahre zurück, eine Methode zum effektiven Training tiefer Netze ist erst 2006 von Hinton \cite{dae2006} entwickelt worden. Zhao \cite{aed2016} hat 2016 einen Autoencoder entworfen, der einen Deskriptor aus Gradienten eines Bildes erzeugt. In einem Experiment mit den Mikolajczyk-Bilddaten\footnote{Hier handelt es sich um Bilder, zu denen transformierte Versionen vorliegen (verzerrt, rotiert, skaliert, etc.). So kann getestet werden, ob ein Deskriptor, unter verschiedenen Bedingungen, die gleichen Features erkennt.} wurden die Ergebnisse mit denen von Lowes SIFT-Deskriptor verglichen. Der Deskriptor schneidet ebenso gut ab wie SIFT, ist aber ca. $3,5$ mal kleiner.\newline
Hinton et al. haben 2017 \cite{drc2017} ihr Konzept über das \enquote{dynamische Routen zwischen Kapseln} veröffentlicht. In diesem Ansatz wird die Position von Objekten, bzw. Teilen dieser, zueinander berücksichtigt. Bisherigen Verfahren können dies nicht leisten: Sie erkennen ob ein Objekt in zwei verschiedenen Bilder vorliegt, ist das Objekt in Bild eins gegenüber dem in Bild zwei um $45${\degree} gedreht, ist diese Information nicht verfügbar. Eine Kapsel (\textit{capusle}) vertritt in einem neuronalen Netzwerk eine Gruppe von Neuronen, die einen Teil eines Objektes repräsentieren. Diese Gruppen werden trainiert und aktivieren, abhängig von ihrer Eingabe, weitere Kapseln. Auf diese Weise werden Objekte aus verschiedenen Konfigurationen von Kapseln modelliert.
% https://arxiv.org/pdf/1710.09829.pdf

\subsection{Auswahl des Modells}

Um die Features zu gruppieren, soll ein k-means Clustering-Verfahren verwendet werden. Anhand der durch k-means gewonnenen Cluster kann eine Histogrammdarstellung für Features erzeugt werden. Die Kombination dieser Verfahren wird Bag of Visual Words genannt und lehnt sich an das Bag of Words-Modell an. K-means ist einer der einfachsten Vertreter der Clustering-Algorithmen, doch die Adaptierung zur Ausführung auf Grafikkarten bringt eine hohe technische Komplexität mit (Speicherverwaltung auf \textit{host} und \textit{device}, Wahl der Blockdimension, Threadanzahl etc.). Bevor also ein anspruchsvollerer Clustering-Algorithmus in Betracht gezogen wird, soll zunächst die Realisierung des k-means-Algorithmus gelingen. Das parallele Verarbeiten von Histogrammen ist ein Lehrbuchbeispiel für den Einsatz von Grafikkarten, da es durch parallele Reduzierung, ein Muster für einige Probleme, erreicht werden kann. \newline
Für die Kompression der Features soll hier ein Autoencoder genutzt werden. Ein mehrstufiger Autoencoder kann mit jeder Schicht einen kompakteren Deskriptor erzeugen und kodiert die gelernten Informationen in den Gewichten. Der Autoencoder bringt darüber hinaus den Vorteil mit sich, das seine Architektur bereits auf eine parallele Verarbeitung ausgelegt ist und nicht \enquote{extra} berücksichtigt werden muss\footnote{Natürlich ist es auch möglich einen rein sequentiellen Autoencoder zu entwickeln, doch da im Wesentlichen nur Matrixoperationen stattfinden, ist eine Ausführung durch Grafikkarten naheliegend.}.

\section{Feature Detektion und Deskription}
\label{extraction}

Für die weitere Verarbeitung der Features ist es erstrebenswert, dass ihre Darstellung möglichst kompakt ist. Deskriptoren werden als Vektoren von Zahlen kodiert, die abhängig vom Verfahren Informationen über einen Pixel und seine Nachbarschaft oder auch ein ganzes Bild enthalten. Je größer die Anzahl der Einträge eines Vektors, desto größer wird der Speicherbedarf und Rechenaufwand.
Die erste Stufe des vorgestellten Modells sieht daher die Komprimierung der Feature-Vektoren durch einen Autoencoder vor. Auf diese Weise kann ein initial recht umfangreicher Feature-Vektor aufgebaut werden: Jede Stufe des Autoencoders lernt dann eine kompaktere Darstellung des Feature-Vektors bis zu einer gewünschten Untergrenze.\newline
Da für die vorliegenden Bilddaten der HsH keine speziellen Annahmen getroffen werden können, ist nicht bekannt was für eine Art von Deskriptor gute Ergebnisse liefern kann. In der Literatur findet sich eine große Anzahl an Verfahren zur Detektion und Extraktion von Features für etliche Zwecke. Um einen Überblick zu geben, sollen einige Vertreter angeführt werden, um den Leser einzuführen.

\subsection{Detektoren}

Feature-Detektoren für Bilder sind in die Kategorien \textit{single-scale},\textit{multi-scale} und \textit{affine invariant} eingeteilt. Detektoren berücksichtigen im Allgemeinen Transformationen wie Rotationen oder Verschiebungen sowie Variationen in der Beleuchtung. Die \textit{multi-scale} Detektoren berücksichtigen zusätzlich Änderungen im Maßstab. Liegen also zwei Bilder vor, die das gleiche Objekt in unterschiedlicher Größe zeigen, werden die gleichen \textit{keypoints} gefunden. Da nicht die Annahme getroffen werden kann, dass die Objekte in den Daten der HsH im gleichen Maßstab vorliegen, liegt hier der Fokus auf \textit{multi-scale} Detektoren.\newline
Die populärsten \textit{multi-scale} Detektoren sind der \textit{Laplacian of Gaussians (Log)} und \textit{Difference of Gaussians}. Ersterer berechnet hierfür die zweite Ableitung der Komponenten und ist daher sehr empfindlich gegenüber Rauschen. In der Praxis wird das Bild bei Verwendung des \textit{LoG} aus diesem Grund zunächst geglättet (das Rauschen reduziert). Da die Glättung ebenfalls eine Faltung des Bildes bedeutet und die Faltung assoziativ ist, wird meist erst der \textit{LoG} mit dem Filter zu Glättung gefaltet: Die Matritzen des \textit{LoG} und Glättungsfilters sind um ein vielfaches kleiner als das Bild und eine Faltung des ganzen Bildes muss so nur einmal statt zweimal erfolgen. Der \textit{DoG} ist eine von Lowe entwickelte Alternative zum \textit{LoG}. Dieser Algorithmus ist zwar nicht genauso präzise, erreicht aber in kürzerer Zeit eine Annäherung die \enquote{gut genug} ist. Da die Bilder der verschiedenen Oktaven im \textit{scale space} (siehe Grundlagen) voneinander subtrahiert werden, ist hier keine Faltung notwendig. Der \textit{DoG} fungiert so als Bandfilter: Es werden Details in den hohen Frequenzen und Flächen in den tiefen Frequenzen gefiltert. Dadurch resultiert auch ein Verlust von Kontrast im Bild.

\subsection{Deskriptoren}

Hier werden einige ausgewählte Deskriptoren vorgestellt, die auf unterschiedliche Anwendungsfälle ausgelegt sind. Der \textit{Spatial Envelope} beurteilt beispielsweise die \enquote{Art} einer Szene, die Local Binary Patterns werden vorwiegend zur Gesichtserkennung verwendet. \newline

\textbf{Local Binary Patterns} Die Local Binary Patterns (LBP) kodieren eine Nachbarschaft eines Pixels, also einen lokalen Teil eines Bildes, indem der Pixel mit seinem Nachbarn verglichen wird. Klassisch wird hier einer $3 \times 3$ Matrix verwendet, sodass sich acht Werte und somit 256 mögliche Kodierungen ergeben.
Praktisch erzielt der Einsatz von LBP vor allem im Bereich der Gesichtserkennung und Erkennung von Nummernschildern gute Ergebnisse. Durch die kleine $3 \times 3$ Matrix werden gerade feine Details berücksichtigt, allerdings können dadurch keine makroskopischen Zusammenhänge berücksichtigt werden. Hierfür können auch größere Nachbarschaften gewählt werden, allerdings gehen dann die Details verloren.\newline 

\textbf{Spatial Envelope} In diesem Ansatz wird davon ausgegangen, dass Menschen eine Szene auch einordnen können, wenn diese in geringer Auflösung vorliegt. Der \textit{Spatial Envelope} beschreibt daher das Bild durch globale Features. Torralba und Olivia \cite{mts2001} haben mit dem  \textit{Spatial Envelope} ein Verfahren entwickelt, um die z.B. die Natürlichkeit oder Offenheit einen Szene zu beurteilen. Eine hohe Natürlichkeit weist zum Beispiel auf das Bild einer Landschaft hin: Hier kommen in der Regel kaum gerade vertikale und horizontale Linien vor, im Gegensatz zu Bildern, die von Menschen angefertigt wurden.\newline

\textbf{SIFT} Der 1999 von Lowe entwickelte SIFT-Deskriptor, ist der einer der häufigst genutzten für die Objekterkennung in Bildern. Der Deskriptor besitzt zwar keine affine Invarianz, in praktischen Anwendungen werden jedoch auch mit skalierten, rotierten und verschobenen Objekten gute Ergebnisse erzielt.
Der mathematische Hintergrund von SIFT wurde wurde bereits im Grundlagen behandelt. Mikolajczyk und Schmid \cite{idp2005} haben 2005 SIFT mit anderen Deskriptoren (u.a. shape context, komplexe Filter, gradient location and orientation histogram, moment invariants, ...) verglichen und kamen zu dem Ergebnis, dass SIFT sehr gut hinsichtlich der Präzison abschneidet. Die Konstruktion des Deskriptors ist allerdings aufwändig und die Beschreibung erfordert einen Feature-Vektor mit 128 Komponenten.\newline

\subsection{Features in dieser Arbeit}

In dieser Arbeit wird das Gruppieren der Bilder durch eine Objekterkennung realisiert. Hiermit eignen sich vor allem \textit{multi-scale} Detektoren. Bei der Auswahl des Deskriptors scheint SIFT, aufgrund der praktischen Erfolge, naheliegend. Die Frage ist, ob bei der Verwendung von SIFT eine weitere Komprimierung noch sinnhaft ist. Die Neuronenanzahl der Schichten eines Stacked Autoencoders nehmen strikt ab, um die Komprimierung zu erreichen. Dies führt dazu, dass zwischen \textit{Input} und dem folgenden \textit{Hidden Layer} maximal $128 \times 127 = 16256$ Verbindungen existieren können. Zhao \cite{aed2016} hat aus diesem Grund einen anderen Ansatz gewählt: Die \textit{keypoints} werden zunächst, wie bei SIFT, durch den DoG-Operator ermittelt. Anschließend wird ein großer Deskriptor aus den Gradienten der Nachbarschaften um die \textit{keypoints} erzeugt, der dann als Eingabe für den Autoencoder dient. Dieser komprimiert den Vektor auf 36 Komponenten. Gegenüber SIFT ist diese Darstellung ca. $3,5$ mal kleiner, was einen nicht unerheblichen Teil, gerade wegen der großen Menge an Features, ausmacht.\newline
Aus diesem Grund sollen zwei Varianten in der Konzeption verfolgt werden: Zum einen die Verwendung von SIFT-Features und zum anderen das Erzeugen des Deskriptors nach Zhao durch einen Autoencoder. Durch die Kompaktheit des Letzteren ist zu erwarten, dass der Clustering-Vorgang bei diesem um ein vielfaches schneller abgeschlossen ist. Dabei war die Qualität der Ergebnisse von Bildervergleichen in Zhaos Test sehr ähnlich. Ob dies auch für den Bag of Visual Words der Fall ist, soll ein Experiment zeigen.